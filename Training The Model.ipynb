{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-13T10:43:18.287009Z",
     "iopub.status.busy": "2023-08-13T10:43:18.286602Z",
     "iopub.status.idle": "2023-08-13T10:43:18.320436Z",
     "shell.execute_reply": "2023-08-13T10:43:18.319658Z",
     "shell.execute_reply.started": "2023-08-13T10:43:18.286980Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:43:57.855382Z",
     "iopub.status.busy": "2023-08-13T10:43:57.855104Z",
     "iopub.status.idle": "2023-08-13T10:43:59.063152Z",
     "shell.execute_reply": "2023-08-13T10:43:59.061708Z",
     "shell.execute_reply.started": "2023-08-13T10:43:57.855357Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('archive/Resume/Resume.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk, Dataset\n",
    "# dataset=Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset=dataset.remove_columns('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.save_to_disk('Chunked Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "dataset=load_from_disk('Chunked Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:43:59.065781Z",
     "iopub.status.busy": "2023-08-13T10:43:59.065446Z",
     "iopub.status.idle": "2023-08-13T10:43:59.079921Z",
     "shell.execute_reply": "2023-08-13T10:43:59.078753Z",
     "shell.execute_reply.started": "2023-08-13T10:43:59.065753Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Short String</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hr administrator marketing associate hr admini...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ication accomplished trainer cross server hosp...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plans participation insurance pension plan per...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted official company page facebook facilitate ...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>egarding medical billing assistant general man...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21635</th>\n",
       "      <td>order create excel spread sheets reports mana...</td>\n",
       "      <td>AVIATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21636</th>\n",
       "      <td>common carriers prepare overseas containers s...</td>\n",
       "      <td>AVIATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21637</th>\n",
       "      <td>osts goods acquired purchase order career deve...</td>\n",
       "      <td>AVIATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21638</th>\n",
       "      <td>er naval aircraft squadron vs received general...</td>\n",
       "      <td>AVIATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21639</th>\n",
       "      <td>minipac poly shrink film packaging systems re...</td>\n",
       "      <td>AVIATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21640 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Short String    labels\n",
       "0      hr administrator marketing associate hr admini...        HR\n",
       "1      ication accomplished trainer cross server hosp...        HR\n",
       "2      plans participation insurance pension plan per...        HR\n",
       "3      ted official company page facebook facilitate ...        HR\n",
       "4      egarding medical billing assistant general man...        HR\n",
       "...                                                  ...       ...\n",
       "21635   order create excel spread sheets reports mana...  AVIATION\n",
       "21636   common carriers prepare overseas containers s...  AVIATION\n",
       "21637  osts goods acquired purchase order career deve...  AVIATION\n",
       "21638  er naval aircraft squadron vs received general...  AVIATION\n",
       "21639   minipac poly shrink film packaging systems re...  AVIATION\n",
       "\n",
       "[21640 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFORMATION-TECHNOLOGY    1204\n",
       "HEALTHCARE                1117\n",
       "CONSULTANT                1100\n",
       "ADVOCATE                  1069\n",
       "ENGINEERING               1062\n",
       "PUBLIC-RELATIONS          1061\n",
       "HR                        1044\n",
       "FINANCE                   1036\n",
       "ACCOUNTANT                1035\n",
       "CONSTRUCTION              1026\n",
       "BUSINESS-DEVELOPMENT      1012\n",
       "AVIATION                  1001\n",
       "BANKING                    986\n",
       "CHEF                       960\n",
       "FITNESS                    904\n",
       "ARTS                       850\n",
       "SALES                      841\n",
       "DESIGNER                   833\n",
       "DIGITAL-MEDIA              814\n",
       "APPAREL                    801\n",
       "TEACHER                    785\n",
       "AGRICULTURE                565\n",
       "AUTOMOBILE                 314\n",
       "BPO                        220\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:43:59.082448Z",
     "iopub.status.busy": "2023-08-13T10:43:59.082145Z",
     "iopub.status.idle": "2023-08-13T10:43:59.089322Z",
     "shell.execute_reply": "2023-08-13T10:43:59.088627Z",
     "shell.execute_reply.started": "2023-08-13T10:43:59.082421Z"
    }
   },
   "outputs": [],
   "source": [
    "# from string import punctuation\n",
    "# print(punctuation)\n",
    "\n",
    "# import re\n",
    "\n",
    "# def cleanResume(resumeText):\n",
    "#     resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
    "#     resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) # remove non-ascii characters\n",
    "#     resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
    "#     resumeText = re.sub(r'[0-9]+', '', resumeText)  #remove numbers\n",
    "#     return resumeText.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:43:59.090878Z",
     "iopub.status.busy": "2023-08-13T10:43:59.090418Z",
     "iopub.status.idle": "2023-08-13T10:44:00.164432Z",
     "shell.execute_reply": "2023-08-13T10:44:00.163110Z",
     "shell.execute_reply.started": "2023-08-13T10:43:59.090849Z"
    }
   },
   "outputs": [],
   "source": [
    "# df[\"Cleaned Resume\"] = df[\"Resume_str\"].apply(lambda x: cleanResume(x))\n",
    "# len(df[\"Cleaned Resume\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:44:00.166435Z",
     "iopub.status.busy": "2023-08-13T10:44:00.165912Z",
     "iopub.status.idle": "2023-08-13T10:44:01.405314Z",
     "shell.execute_reply": "2023-08-13T10:44:01.403878Z",
     "shell.execute_reply.started": "2023-08-13T10:44:00.166404Z"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# import string\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk import word_tokenize\n",
    "\n",
    "# len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:44:01.407890Z",
     "iopub.status.busy": "2023-08-13T10:44:01.406542Z",
     "iopub.status.idle": "2023-08-13T10:44:01.417938Z",
     "shell.execute_reply": "2023-08-13T10:44:01.416776Z",
     "shell.execute_reply.started": "2023-08-13T10:44:01.407842Z"
    }
   },
   "outputs": [],
   "source": [
    "# df=df.drop(['Resume_html','Resume_str'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:44:01.419604Z",
     "iopub.status.busy": "2023-08-13T10:44:01.419236Z",
     "iopub.status.idle": "2023-08-13T10:44:01.436594Z",
     "shell.execute_reply": "2023-08-13T10:44:01.435472Z",
     "shell.execute_reply.started": "2023-08-13T10:44:01.419551Z"
    }
   },
   "outputs": [],
   "source": [
    "# pd.set_option('max_colwidth', 800)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# def remove_stop_words(sentence): \n",
    "#   words = sentence.split() \n",
    "#   filtered_words = [word for word in words if word not in stop_words] \n",
    "#   return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Cleaned Resume\"] = df[\"Cleaned Resume\"].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "has_nan = df['Short String'].isna().any().any()\n",
    "has_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2=df.dropna(subset=['Short String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.to_csv(\"Cleaned Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:44:01.442327Z",
     "iopub.status.busy": "2023-08-13T10:44:01.441664Z",
     "iopub.status.idle": "2023-08-13T10:44:01.452461Z",
     "shell.execute_reply": "2023-08-13T10:44:01.451631Z",
     "shell.execute_reply.started": "2023-08-13T10:44:01.442298Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_to_id = {}\n",
    "id_to_label = {}\n",
    "\n",
    "for idx, label in enumerate(df.labels.unique()):\n",
    "    labels_to_id[label] = idx\n",
    "    id_to_label[idx] = label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:44:01.454434Z",
     "iopub.status.busy": "2023-08-13T10:44:01.453903Z",
     "iopub.status.idle": "2023-08-13T10:44:12.287822Z",
     "shell.execute_reply": "2023-08-13T10:44:12.286443Z",
     "shell.execute_reply.started": "2023-08-13T10:44:01.454388Z"
    }
   },
   "outputs": [],
   "source": [
    "# ds_df = df[df.Category == id_to_label[5]]\n",
    "# nltk.download('punkt')\n",
    "# resumes=\"\"\n",
    "# total_words = []\n",
    "# for resume in ds_df[\"Cleaned Resume\"]:\n",
    "#     resumes += resume\n",
    "#     words = word_tokenize(resume)\n",
    "#     for word in words :\n",
    "#         if word not in set(stopwords.words('english')) and word not in string.punctuation:\n",
    "#             total_words.append(word)\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "# wordcloudimage = WordCloud( font_step=2 ,\n",
    "#                             max_font_size=500,\n",
    "#                             collocations = False,\n",
    "#                             #collocation_threshold = 1 \n",
    "#                           ).generate(resumes)\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# plt.imshow(wordcloudimage, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:44:12.289690Z",
     "iopub.status.busy": "2023-08-13T10:44:12.289353Z",
     "iopub.status.idle": "2023-08-13T10:44:12.296252Z",
     "shell.execute_reply": "2023-08-13T10:44:12.295143Z",
     "shell.execute_reply.started": "2023-08-13T10:44:12.289662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BUSINESS-DEVELOPMENT'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:44:12.298166Z",
     "iopub.status.busy": "2023-08-13T10:44:12.297765Z",
     "iopub.status.idle": "2023-08-13T10:44:27.765227Z",
     "shell.execute_reply": "2023-08-13T10:44:27.764256Z",
     "shell.execute_reply.started": "2023-08-13T10:44:12.298130Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, BartTokenizerFast\n",
    "\n",
    "checkpoint =\"training-new\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:44:27.767761Z",
     "iopub.status.busy": "2023-08-13T10:44:27.767081Z",
     "iopub.status.idle": "2023-08-13T10:44:27.771276Z",
     "shell.execute_reply": "2023-08-13T10:44:27.770476Z",
     "shell.execute_reply.started": "2023-08-13T10:44:27.767732Z"
    }
   },
   "outputs": [],
   "source": [
    "# def tokenize_function(example):\n",
    "#     tokenized = tokenizer(example['Cleaned Resume'], truncation=True, padding=True, return_tensors='pt')\n",
    "#     label = labels_to_id[example['Category']]\n",
    "#     return {'tokenized':tokenized,'labels':label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:44:27.772421Z",
     "iopub.status.busy": "2023-08-13T10:44:27.772134Z",
     "iopub.status.idle": "2023-08-13T10:44:27.793273Z",
     "shell.execute_reply": "2023-08-13T10:44:27.792265Z",
     "shell.execute_reply.started": "2023-08-13T10:44:27.772394Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenized={}\n",
    "# def tokenize_function(example):\n",
    "#     tokenized = tokenizer(example['Cleaned Resume'],return_tensors='pt',is_split_into_words= True)\n",
    "# #     tokenized['attention_mask'] = tokenizer(example['Cleaned Resume'],return_tensors='pt',is_split_into_words= True)['attention_mask'].squeeze(0)\n",
    "# #     tokenized['length'] = len(tokenized['input_ids'])\n",
    "#     label = labels_to_id[example['Category']]\n",
    "#     return {'input_ids':tokenized['input_ids'].squeeze(0),'attention_mask':tokenized['attention_mask'].squeeze(0),'labels':label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Short String', 'labels'],\n",
       "    num_rows: 21640\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:45:14.851255Z",
     "iopub.status.busy": "2023-08-13T10:45:14.850931Z",
     "iopub.status.idle": "2023-08-13T10:45:14.858451Z",
     "shell.execute_reply": "2023-08-13T10:45:14.856807Z",
     "shell.execute_reply.started": "2023-08-13T10:45:14.851231Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return (tokenizer(example['Short String'],max_length=128,truncation=True,return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substring_length = 600\n",
    "\n",
    "# def split_string(example):\n",
    "#     labels=example['Category']\n",
    "#     long_string=example['Cleaned Resume']\n",
    "#     shorts = [long_string[i:i+substring_length] for i in range(0, len(long_string), substring_length)]\n",
    "#     return {'Short String':shorts,'labels':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:45:16.967433Z",
     "iopub.status.busy": "2023-08-13T10:45:16.967072Z",
     "iopub.status.idle": "2023-08-13T10:45:16.991419Z",
     "shell.execute_reply": "2023-08-13T10:45:16.990643Z",
     "shell.execute_reply.started": "2023-08-13T10:45:16.967400Z"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# dataset=Dataset.from_pandas(df)\n",
    "# df2=dataset.map(split_string,remove_columns=dataset.column_names)\n",
    "# df3 = pd.DataFrame(df2)\n",
    "# df3 = df3.explode('Short String')\n",
    "# dataset = Dataset.from_pandas(df3)\n",
    "# dataset=dataset.remove_columns('__index_level_0__')\n",
    "# dataset.save_to_disk('Chunked Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:45:18.602733Z",
     "iopub.status.busy": "2023-08-13T10:45:18.601650Z",
     "iopub.status.idle": "2023-08-13T10:45:18.607497Z",
     "shell.execute_reply": "2023-08-13T10:45:18.606770Z",
     "shell.execute_reply.started": "2023-08-13T10:45:18.602700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def label_encode(example):\n",
    "    example['input_ids']=torch.tensor(example['input_ids']).squeeze(0)\n",
    "    example['attention_mask']=torch.tensor(example['attention_mask']).squeeze(0)\n",
    "    example['token_type_ids']=torch.tensor(example['token_type_ids']).squeeze(0)\n",
    "    \n",
    "    example['label'] = labels_to_id[example['labels']]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:45:20.608426Z",
     "iopub.status.busy": "2023-08-13T10:45:20.608020Z",
     "iopub.status.idle": "2023-08-13T10:45:29.039718Z",
     "shell.execute_reply": "2023-08-13T10:45:29.038755Z",
     "shell.execute_reply.started": "2023-08-13T10:45:20.608396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset=dataset.map(tokenize_function)#remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:45:29.042089Z",
     "iopub.status.busy": "2023-08-13T10:45:29.041735Z",
     "iopub.status.idle": "2023-08-13T10:45:31.973514Z",
     "shell.execute_reply": "2023-08-13T10:45:31.972207Z",
     "shell.execute_reply.started": "2023-08-13T10:45:29.042061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset=dataset.map(label_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:45:31.975604Z",
     "iopub.status.busy": "2023-08-13T10:45:31.975283Z",
     "iopub.status.idle": "2023-08-13T10:45:31.985492Z",
     "shell.execute_reply": "2023-08-13T10:45:31.983936Z",
     "shell.execute_reply.started": "2023-08-13T10:45:31.975576Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_columns=['Short String', 'labels']\n",
    "dataset=dataset.remove_columns(remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:45:35.371869Z",
     "iopub.status.busy": "2023-08-13T10:45:35.371542Z",
     "iopub.status.idle": "2023-08-13T10:45:35.392799Z",
     "shell.execute_reply": "2023-08-13T10:45:35.391633Z",
     "shell.execute_reply.started": "2023-08-13T10:45:35.371843Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset=dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T10:45:35.394540Z",
     "iopub.status.busy": "2023-08-13T10:45:35.394200Z",
     "iopub.status.idle": "2023-08-13T10:45:35.405637Z",
     "shell.execute_reply": "2023-08-13T10:45:35.404459Z",
     "shell.execute_reply.started": "2023-08-13T10:45:35.394511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader= DataLoader(\n",
    "    dataset['train'],\n",
    "    shuffle= True,\n",
    "    collate_fn= data_collator,\n",
    "    batch_size= 12,\n",
    ")\n",
    "\n",
    "eval_dataloader= DataLoader(\n",
    "    dataset['test'],\n",
    "    collate_fn= data_collator,\n",
    "    batch_size= 12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification \n",
    "model= AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label= id_to_label,\n",
    "    label2id= labels_to_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= AdamW(model.parameters(), lr= 5e-5)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs= 30\n",
    "num_update_steps_per_epoch= len(train_dataloader)\n",
    "num_training_steps= num_train_epochs * num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, get_scheduler\n",
    "\n",
    "lr_scheduler= get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer= optimizer,\n",
    "    num_warmup_steps= 0,\n",
    "    num_training_steps= num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.mkdir(\"./training-new1\")\n",
    "output_dir= \"./training-new1/\"\n",
    "os.listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0c7a6a6b21482183d0c6f4f5f5f978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48690 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'accuracy': tensor(0.7159, device='cuda:0'), 'loss': tensor(1.0379, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 1: {'accuracy': tensor(0.6901, device='cuda:0'), 'loss': tensor(1.1493, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 2: {'accuracy': tensor(0.6952, device='cuda:0'), 'loss': tensor(1.2039, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 3: {'accuracy': tensor(0.6791, device='cuda:0'), 'loss': tensor(1.2954, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 4: {'accuracy': tensor(0.6952, device='cuda:0'), 'loss': tensor(1.2880, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 5: {'accuracy': tensor(0.6763, device='cuda:0'), 'loss': tensor(1.4516, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 6: {'accuracy': tensor(0.6961, device='cuda:0'), 'loss': tensor(1.4457, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 7: {'accuracy': tensor(0.6823, device='cuda:0'), 'loss': tensor(1.5667, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 8: {'accuracy': tensor(0.6897, device='cuda:0'), 'loss': tensor(1.4957, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 9: {'accuracy': tensor(0.6994, device='cuda:0'), 'loss': tensor(1.5768, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 10: {'accuracy': tensor(0.6763, device='cuda:0'), 'loss': tensor(1.7637, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 11: {'accuracy': tensor(0.6763, device='cuda:0'), 'loss': tensor(1.9022, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 12: {'accuracy': tensor(0.6846, device='cuda:0'), 'loss': tensor(1.8877, device='cuda:0')} \n",
      "\n",
      "\n",
      "epoch 13: {'accuracy': tensor(0.6957, device='cuda:0'), 'loss': tensor(1.7859, device='cuda:0')} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    current_loss= []\n",
    "    accuracy= []\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        \n",
    "\n",
    "        predictions = outputs.logits.argmax(dim= -1)\n",
    "        labels = batch[\"labels\"]\n",
    "        loss= loss_fn(outputs['logits'],labels)\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "#         predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "#         labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "#         predictions_gathered = accelerator.gather(predictions)\n",
    "#         labels_gathered = accelerator.gather(labels)\n",
    "        current_loss.append(loss)\n",
    "        acc= sum(predictions == labels) / len(labels)\n",
    "        accuracy.append(acc)\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    results = sum(accuracy)/ len(accuracy) #avg accuracy #metric.compute()\n",
    "    loss= sum(current_loss)/ len(current_loss)\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            \"accuracy\": results,\n",
    "            \"loss\" : loss\n",
    "        }, \"\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
